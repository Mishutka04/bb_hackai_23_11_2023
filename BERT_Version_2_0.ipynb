{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка необходимых библиотек\n"
      ],
      "metadata": {
        "id": "s53OEbLjAA42"
      },
      "id": "s53OEbLjAA42"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z_G4o7FcA0g4"
      },
      "id": "z_G4o7FcA0g4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37dd8619-49dd-4d69-8b4b-dbf1a102c2bf",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T09:30:25.322430Z",
          "iopub.status.busy": "2023-11-25T09:30:25.321518Z",
          "iopub.status.idle": "2023-11-25T09:30:54.851492Z"
        },
        "tags": [],
        "id": "37dd8619-49dd-4d69-8b4b-dbf1a102c2bf",
        "outputId": "070c99e4-70a2-4fc1-acce-bda303796eb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m696.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (2023.7.22)\n",
            "Collecting charset-normalizer~=2.0.0 (from requests<3.0.0,>=2.19.0->deeppavlov)\n",
            "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests<3.0.0,>=2.19.0->deeppavlov) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.1.0,>=0.24->deeppavlov) (3.2.0)\n",
            "Requirement already satisfied: h11>=0.8 in /home/jupyter/.local/lib/python3.10/site-packages (from uvicorn<0.19.0,>=0.13.0->deeppavlov) (0.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /kernel/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas<1.6.0,>=1.0.0->deeppavlov) (1.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette==0.22.0->fastapi<=0.89.1,>=0.47.0->deeppavlov) (1.1.2)\n",
            "Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, charset-normalizer\n",
            "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 urllib3-1.26.18\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in /home/jupyter/.local/lib/python3.10/site-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /kernel/lib/python3.10/site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/jupyter/.local/lib/python3.10/site-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /kernel/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests->transformers)\n",
            "  Using cached urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /kernel/lib/python3.10/site-packages (from requests->transformers) (2023.7.22)\n",
            "Collecting charset-normalizer~=2.0.0 (from requests->transformers)\n",
            "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /kernel/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
            "Using cached urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "Installing collected packages: urllib3, charset-normalizer\n",
            "\u001b[33m  WARNING: The script normalizer is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
            "\u001b[0mSuccessfully installed charset-normalizer-2.0.12 urllib3-1.26.18\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /home/jupyter/.local/lib/python3.10/site-packages (from torch) (3.9.1)\n",
            "Requirement already satisfied: typing-extensions in /kernel/lib/python3.10/site-packages (from torch) (4.8.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /kernel/lib/python3.10/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /kernel/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tqdm in /home/jupyter/.local/lib/python3.10/site-packages (4.64.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: fastparquet in /home/jupyter/.local/lib/python3.10/site-packages (2023.10.1)\n",
            "Requirement already satisfied: pandas>=1.5.0 in /home/jupyter/.local/lib/python3.10/site-packages (from fastparquet) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /home/jupyter/.local/lib/python3.10/site-packages (from fastparquet) (1.23.5)\n",
            "Requirement already satisfied: cramjam>=2.3 in /home/jupyter/.local/lib/python3.10/site-packages (from fastparquet) (2.7.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from fastparquet) (2023.6.0)\n",
            "Requirement already satisfied: packaging in /kernel/lib/python3.10/site-packages (from fastparquet) (23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /kernel/lib/python3.10/site-packages (from pandas>=1.5.0->fastparquet) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5.0->fastparquet) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /kernel/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas>=1.5.0->fastparquet) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install deeppavlov\n",
        "%pip install transformers\n",
        "%pip install torch\n",
        "%pip install tqdm\n",
        "%pip install fastparquet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели на основании данных авторазметки\n",
        "Описание: реализация совершенно новыго подхода к обучению модели с использованием таких данных как:\n",
        "\n",
        "1. Время промотра\n",
        "2. Позиция в списке выдачи\n",
        "3. Эмоции\n",
        "4. В топ"
      ],
      "metadata": {
        "id": "HnaKiQ1YAHuh"
      },
      "id": "HnaKiQ1YAHuh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30932590-10fe-4443-8d3a-606bb46c2047",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-11-25T09:41:24.666411Z",
          "iopub.status.busy": "2023-11-25T09:41:24.665193Z",
          "iopub.status.idle": "2023-11-25T09:43:11.831348Z",
          "shell.execute_reply": "2023-11-25T09:43:11.830471Z",
          "shell.execute_reply.started": "2023-11-25T09:41:24.666365Z"
        },
        "tags": [],
        "id": "30932590-10fe-4443-8d3a-606bb46c2047",
        "outputId": "c3fda8df-7693-4c3e-940c-f84fbc3e1124"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/home/jupyter/.local/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/1: 100%|██████████| 40/40 [01:20<00:00,  2.01s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/1, Loss: 0.1917418352793902\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 10/10 [00:18<00:00,  1.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9997\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('BERT_V2.0_100000/tokenizer_config.json',\n",
              " 'BERT_V2.0_100000/special_tokens_map.json',\n",
              " 'BERT_V2.0_100000/vocab.txt',\n",
              " 'BERT_V2.0_100000/added_tokens.json')"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Определение класса CustomDataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text_dict = self.data.iloc[idx][['query', 'video_id', 'duration', 'position', 'watchtime', 'emotion']]\n",
        "        label = self.data.iloc[idx]['vtop']\n",
        "\n",
        "        # Проверка на None и установка значения по умолчанию\n",
        "        label = 0 if label is None else int(label)\n",
        "\n",
        "        # Преобразование текста в строку\n",
        "        text = ' '.join(map(str, text_dict.values))\n",
        "\n",
        "        return {'text': text, 'label': label}\n",
        "\n",
        "# Указать путь к файлу Parquet\n",
        "data_path = 'train_data/train_data/automarkup.parquet'\n",
        "\n",
        "# Загрузить данные из файла Parquet\n",
        "df = pd.read_parquet(data_path)[:50000]\n",
        "\n",
        "# Замените NaN в 'emotion' на значение по умолчанию, например, 'нет эмоции'\n",
        "df['emotion'].fillna('нет эмоции', inplace=True)\n",
        "\n",
        "# Предполагаем, что 'duration' измеряется в миллисекундах, преобразуем в секунды\n",
        "df['duration'] = df['duration'] / 1000\n",
        "\n",
        "# Разделите данные на обучающий и тестовый наборы\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Создайте DataLoader\n",
        "train_dataset = CustomDataset(train_df)\n",
        "test_dataset = CustomDataset(test_df)\n",
        "train_loader = DataLoader(train_dataset, batch_size=2000, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2000, shuffle=False)\n",
        "\n",
        "# Загрузите предварительно обученный токенизатор и модель BERT для русского языка\n",
        "model_name = 'cointegrated/rubert-tiny2'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # 2 класса: в топе или нет\n",
        "\n",
        "# Настройте оптимизатор и функцию потерь\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Обучение модели\n",
        "num_epochs = 1\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
        "        inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(**inputs).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss}\")\n",
        "\n",
        "# Оценка модели\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "for batch in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "    inputs = tokenizer(batch['text'], return_tensors='pt', padding=True, truncation=True).to(device)\n",
        "    labels = batch['label'].to(device)\n",
        "\n",
        "    outputs = model(**inputs).logits\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    all_predictions.extend(predictions.cpu().numpy())\n",
        "    all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(all_labels, all_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Сохранение модели\n",
        "model.save_pretrained(\"BERT_V2.0_100000\")\n",
        "tokenizer.save_pretrained(\"BERT_V2.0_100000\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f780e5-8a4c-46bc-8671-f9c0c1e69bb8",
      "metadata": {
        "id": "97f780e5-8a4c-46bc-8671-f9c0c1e69bb8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DataSphere Kernel",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}